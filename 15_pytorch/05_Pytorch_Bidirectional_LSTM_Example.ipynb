{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # For all optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.utils.data import DataLoader # Gives easier dataset management and creates mini batches\n",
    "import torchvision.datasets as datasets # Has standard datasetswe can import in a nice way\n",
    "import torchvision.transforms as transforms # Transformations we can perform our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "sequence_length = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bi-directional LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layer*2, x.size(0), self.hidden_size ).to(device)\n",
    "\n",
    "        c0 = torch.zeros(self.num_layer*2, x.size(0), self.hidden_size ).to(device)\n",
    "        \n",
    "        out, (hidden_state, cell_state) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) # sending last hidden state of model  \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to make `LSTM` bidiectional just keeping that parameter as `bidirectional = True`\n",
    "\n",
    "- `self.num_layers x 2` just so that we can several number of layers time by 2, since one of them going forward and one of them going backward but they are all gone get concatinated in the same specific hidden state, so we just to exand this tensor by 2 in 2 axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalize Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  BRNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=False)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average Loss: 0.4211287443786224\n",
      "Epoch 1: Average Loss: 0.09959488496162505\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_size, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device).squeeze(1)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        scores = model(data)\n",
    "\n",
    "        loss = criterion(scores, target)\n",
    "\n",
    "        # backward\n",
    "\n",
    "        model.zero_grad() # clear previous gradients\n",
    "\n",
    "        loss.backward() # back propogation\n",
    "\n",
    "        optimizer.step() # update mode weights\n",
    "\n",
    "        total_loss += loss.item() # Accumulate loss\n",
    "\n",
    "    average_loss = total_loss / len(train_loader) # Calculating average loss\n",
    "\n",
    "    print(f\"Epoch {epoch}: Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy on train & test see how good our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, mode):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on testing data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_sample = 0\n",
    "    model.eval()  # put model in evaluation model\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for x, y in loader:\n",
    "\n",
    "            x = x.to(device).squeeze(1)\n",
    "            y = y.to(device)\n",
    "\n",
    "            scores = model(x)\n",
    "\n",
    "            _, predictions = scores.max(1)\n",
    "\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            num_sample += predictions.size(0)\n",
    "\n",
    "        acc = float(num_correct) / float(num_sample) * 100\n",
    "\n",
    "        print(f\"Got {num_correct} / {num_sample} with accuracy {acc:.2f}\")\n",
    "    \n",
    "    model.train() # put model in tarining mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 58912 / 60000 with accuracy 98.19\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on testing data\n",
      "Got 9779 / 10000 with accuracy 97.79\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
